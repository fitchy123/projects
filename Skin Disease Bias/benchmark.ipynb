{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"elpW255QdJZu"},"outputs":[],"source":["# implementation of fine-tuned ResNet-18 based on code from\n","# https://github.com/IBM/star-ed/blob/main/segmentation_and_skintone_classification.ipynb"]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import random\n","import torch\n","from torch import Tensor\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import torch.nn.functional as F\n","from torchvision import transforms, models\n","from torch.utils.data import Dataset\n","from torchvision.io import read_image\n","from skimage import io, color\n","import os\n","from PIL import Image\n","import time, copy\n","from tqdm import tqdm\n","import cv2\n","import matplotlib.pyplot as plt\n","import torchvision"],"metadata":{"id":"deTdOIK8daEY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#from google.colab import drive\n","#drive.mount('/content/drive')"],"metadata":{"id":"oi-_bSwlfWEc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import zipfile\n","# Path in your Google Drive\n","source_path = r'drive/MyDrive/Edinburgh/MLP/MLPcoursework4/image_arrays.zip'\n","# Destination path on the local VM disk\n","os.mkdir(\"/tmp/image_arrays\")\n","arr_zip = zipfile.ZipFile(source_path, \"r\")\n","arr_zip.extractall(\"tmp/image_arrays\")"],"metadata":{"id":"h2cUMeqv2WPp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(\"drive/MyDrive/Edinburgh/MLP/MLPcoursework4/fitzpatrick17k_filtered.csv\")\n","data_path = r\"tmp/image_arrays/\""],"metadata":{"id":"gdFOzFwAq9M5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# useful variables\n","img_height = 256\n","img_width = 256\n","batch_size  = 32\n","n_channels  = 3\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"],"metadata":{"id":"S-80HYS-ewjm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# split dataset\n","df = pd.read_csv(\"drive/MyDrive/Edinburgh/MLP/MLPcoursework4/fitzpatrick17k_filtered.csv\")\n","train_df = df[df[\"validation\"] == 0].copy()\n","train_df = train_df.reset_index(drop=True)\n","valid_df = df[df[\"validation\"] == 1].copy()\n","valid_df = valid_df.reset_index(drop=True)\n","len_train = len(train_df)\n","len_valid = len(valid_df)\n","batches_per_valid_epoch = np.ceil(len_valid / batch_size)"],"metadata":{"id":"xujTxp--fl09"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# segmentation test\n","class SkinDetector(object):\n","    \"\"\"Simple skin segmentation for both with and without lesions\"\"\"\n","\n","    def __init__(self, image):\n","        self.image = image #np.load(imageName) #cv2.imread(imageName)\n","        if self.image is None:\n","            print(\"Image Not Found\")\n","            exit(1)\n","        self.HSV_image = cv2.cvtColor(self.image, cv2.COLOR_BGR2HSV)\n","        self.YCbCr_image = cv2.cvtColor(self.image, cv2.COLOR_BGR2YCR_CB)\n","        self.binary_mask_image = self.HSV_image\n","\n","    def find_skin(self):\n","        \"\"\"function to process the image and segment the skin using\n","        the HSV and YCbCr color spaces, followed by the Watershed algorithm\"\"\"\n","        self.color_segmentation()\n","        image_mask = self.region_based_segmentation()\n","\n","        return image_mask\n","\n","    def color_segmentation(self):\n","        \"\"\"Apply a threshold to an HSV and YCbCr images,\n","        the used values were based on current research papers along with some\n","        empirical tests and visual evaluation\"\"\"\n","        lower_HSV_values = np.array([0, 40, 0], dtype=\"uint8\")\n","        upper_HSV_values = np.array([25, 255, 255], dtype=\"uint8\")\n","\n","        lower_YCbCr_values = np.array((0, 138, 67), dtype=\"uint8\")\n","        upper_YCbCr_values = np.array((255, 173, 133), dtype=\"uint8\")\n","\n","        mask_YCbCr = cv2.inRange(\n","            self.YCbCr_image, lower_YCbCr_values, upper_YCbCr_values\n","        )\n","        mask_HSV = cv2.inRange(self.HSV_image, lower_HSV_values, upper_HSV_values)\n","        self.binary_mask_image = cv2.add(mask_HSV, mask_YCbCr)\n","\n","    def region_based_segmentation(self):\n","        \"\"\"applies Watershed and morphological operations on the thresholded image\"\"\"\n","\n","        image_foreground = cv2.erode(self.binary_mask_image, None, iterations=3)\n","        dilated_binary_image = cv2.dilate(self.binary_mask_image, None, iterations=3)\n","        ret, image_background = cv2.threshold(\n","            dilated_binary_image, 1, 128, cv2.THRESH_BINARY\n","        )\n","        image_marker = cv2.add(image_foreground, image_background)\n","        image_marker32 = np.int32(image_marker)\n","\n","        cv2.watershed(self.image, image_marker32)\n","        m = cv2.convertScaleAbs(image_marker32)\n","\n","        # bitwise of the mask with the input image\n","        ret, image_mask = cv2.threshold(m, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n","        output = cv2.bitwise_and(self.image, self.image, mask=image_mask)\n","\n","        return image_mask, output"],"metadata":{"id":"7lo6DrF0HziY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create dataset class\n","class CustomSkinDataset(Dataset):\n","    def __init__(self, img_dir, csv, transforms=None, colour_space=\"HSV\"):\n","        self.img_dir = img_dir\n","        self.colour_space = colour_space\n","        self.csv = csv\n","        self.transforms = transforms\n","    def __len__(self):\n","        return len(self.csv)\n","\n","    def __getitem__(self, idx):\n","        img = np.load(self.img_dir + self.csv.at[idx, 'md5hash'] + \".npy\")\n","        if(len(img.shape) < 3):\n","            img = color.gray2rgb(img)\n","        if self.colour_space == \"HSV\":\n","            img = cv2.cvtColor(img, cv2.COLOR_RGB2Lab)\n","        if self.transforms:\n","            img = self.transforms(Image.fromarray(img))\n","        label = self.csv[\"fitzpatrick_scale\"][idx] - 1\n","        return img, torch.tensor(label, dtype=torch.long)"],"metadata":{"id":"MZPDSghRhls9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# balanced batch sampler\n","class BalancedBatchSampler(torch.utils.data.sampler.Sampler):\n","    \"\"\"\n","    A pytorch dataset sampler to obtain balanced batches.\n","    Implementation from\n","    https://github.com/galatolofederico/pytorch-balanced-batch\n","    \"\"\"\n","\n","    def __init__(self, dataset, labels=None):\n","        self.labels = labels\n","        self.dataset = dict()\n","        self.balanced_max = 0\n","        # Save all the indices for all the classes\n","        for idx in range(0, len(dataset)):\n","            label = self._get_label(dataset, idx)\n","            if label not in self.dataset:\n","                self.dataset[label] = list()\n","            self.dataset[label].append(idx)\n","            # keep track of number in class with most entries\n","            self.balanced_max = (\n","                len(self.dataset[label])\n","                if len(self.dataset[label]) > self.balanced_max\n","                else self.balanced_max\n","            )\n","        # Oversample the classes with fewer elements than the max, creates balanced classes\n","        for label in self.dataset:\n","            while len(self.dataset[label]) < self.balanced_max:\n","                self.dataset[label].append(random.choice(self.dataset[label]))\n","        self.keys = list(self.dataset.keys())\n","        self.currentkey = 0\n","        self.indices = [-1] * len(self.keys)\n","\n","    def __iter__(self):\n","        i = 0\n","        while self.indices[self.currentkey] < self.balanced_max - 1:\n","            self.indices[self.currentkey] += 1\n","            yield self.dataset[self.keys[self.currentkey]][\n","                self.indices[self.currentkey]\n","            ]\n","            self.currentkey = (self.currentkey + 1) % len(self.keys)\n","            i += 1\n","        self.indices = [-1] * len(self.keys)\n","\n","    def _get_label(self, dataset, idx):\n","        if self.labels is not None:\n","            return self.labels[idx]\n","        else:\n","            return self.csv[\"fitzpatrick_scale\"][idx]\n","\n","    def __len__(self):\n","        return self.balanced_max * len(self.keys)"],"metadata":{"id":"48H9kmmwUyHH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def print_model_parameters(model):\n","    total_params = sum(p.numel() for p in model.parameters())\n","    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    non_trainable_params = total_params - trainable_params\n","\n","    print(f\"Total Parameters: {total_params}\")\n","    print(f\"Trainable Parameters: {trainable_params}\")\n","    print(f\"Non-Trainable Parameters: {non_trainable_params}\")"],"metadata":{"id":"agLMTlim29SR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_transforms = transforms.Compose([\n","            transforms.Resize(256),\n","            transforms.RandomResizedCrop(224),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.RandomVerticalFlip(),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n","valid_transforms = transforms.Compose([\n","            transforms.Resize(256),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n","\n","train_dataset = CustomSkinDataset(data_path, train_df, train_transforms, colour_space=\"RGB\")\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=2, sampler=BalancedBatchSampler(train_dataset, labels=np.array(train_df[\"fitzpatrick_scale\"])))\n","valid_dataset = CustomSkinDataset(data_path, valid_df, valid_transforms, colour_space=\"RGB\")\n","valid_loader = torch.utils.data.DataLoader(valid_dataset, shuffle=False, batch_size=batch_size, num_workers=2)"],"metadata":{"id":"4l-XoJl4CGLG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for img, label in train_loader:\n","    plt.imshow(torchvision.utils.make_grid(img.add(1).mul(0.5)).clamp(0,1).cpu().data.permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\n","    plt.show()\n","    print(label + 1)\n","    break"],"metadata":{"id":"rlLkb2WA_JK5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_ft = models.resnet18(pretrained=True)\n","num_ftrs = model_ft.fc.in_features\n","model_ft.fc = nn.Linear(num_ftrs, 6)\n","baseline_model = model_ft.to(device)\n","\n","loss_function = nn.CrossEntropyLoss()\n","\n","optimiser = optim.SGD(baseline_model.parameters(), lr=0.001, momentum=0.9)\n","\n","exp_lr_scheduler = lr_scheduler.StepLR(optimiser, step_size=10, gamma=0.1)\n","print_model_parameters(model_ft)"],"metadata":{"id":"PsLX3A-RSJl1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, optim, scheduler, train_loader, loss_function, epoch, batch_size):\n","    model.train()\n","    total_loss = 0\n","    for img, label in tqdm(train_loader, desc=f'Epoch {epoch}', total=len(train_loader)):\n","        img = img.to(device)\n","        label = label.to(device)\n","\n","        optim.zero_grad()\n","\n","        output = model(img)\n","        loss = loss_function(output, label)\n","\n","        loss.backward()\n","        optim.step()\n","\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(train_loader)\n","    scheduler.step()\n","    print(f\"Epoch: {epoch} ||| Training loss: {avg_loss}\")"],"metadata":{"id":"JmE2NJSmXSui"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def validate(model, valid_loader, loss_function, epoch, valid_length, batch_size):\n","    model.eval()\n","    total_loss = 0\n","    num_correct, num_give1_correct = 0, 0\n","    for img, label in tqdm(valid_loader, desc=f'Epoch {epoch}', total=len(valid_loader)):\n","        img = img.to(device)\n","        label = label.to(device)\n","\n","        output = model(img)\n","        preds = torch.argmax(output, dim=1)\n","        num_correct += (preds == label).sum()\n","        num_give1_correct += (torch.logical_or(torch.logical_or((preds == label), (preds == (label - 1))), (preds == (label + 1)))).sum()\n","\n","        loss = loss_function(output, label)\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(valid_loader)\n","    acc = num_correct / valid_length\n","    acc_give1 = num_give1_correct / valid_length\n","    print(f\"Validation loss: {avg_loss} ||| Validation accuracy: {acc} ||| Validation +-1 accuracy: {acc_give1}\")\n","    return acc, acc_give1"],"metadata":{"id":"a88MJXpXaMYk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 10\n","best_acc, _ = validate(baseline_model, valid_loader, loss_function, 0, len_valid, batch_size)\n","for epoch in range(epochs):\n","    train(baseline_model, optimiser, exp_lr_scheduler, train_loader, loss_function, epoch+1, batch_size)\n","    acc, acc_give1 = validate(baseline_model, valid_loader, loss_function, epoch+1, len_valid, batch_size)\n","    if acc > best_acc:\n","        torch.save({'model':baseline_model.state_dict(), 'optimiser':optimiser.state_dict(), 'scheduler':exp_lr_scheduler.state_dict(), 'acc': acc, 'acc_give1': acc_give1, 'epoch': epoch+1}, f\"drive/MyDrive/Edinburgh/MLP/MLPcoursework4/models/baseline_epoch-{epoch+1}.chkpt\")\n","        best_acc = acc"],"metadata":{"id":"_TY4OipMfkAf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loaded_model = models.resnet18()\n","num_ftrs = model_ft.fc.in_features\n","loaded_model.fc = nn.Linear(num_ftrs, 6)\n","loaded_model.to(device)\n","checkpoint = torch.load(\"drive/MyDrive/Edinburgh/MLP/MLPcoursework4/models/baseline_epoch-9.chkpt\")\n","loaded_model.load_state_dict(checkpoint['model'])\n","_, _ = validate(loaded_model, valid_loader, loss_function, epoch+1, len_valid, batch_size)"],"metadata":{"id":"aDrVPv4Y80Ax"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def bias_analysis(model, valid_loader):\n","    model.eval()\n","    skintone_counts = np.zeros(6)\n","    skintone_counts_correct = np.zeros(6)\n","    for img, label in tqdm(valid_loader, desc=f'Epoch {epoch}', total=len(valid_loader)):\n","        img = img.to(device)\n","        label = label.to(device)\n","\n","        output = model(img)\n","        preds = torch.argmax(output, dim=1)\n","\n","        label_arr = label.cpu().detach().numpy()\n","        entries, counts = np.unique(label_arr, return_counts=True)\n","        skintone_counts[entries] += counts\n","        pred_arr = preds.cpu().detach().numpy()\n","        ind_corr = np.where(label_arr == pred_arr)\n","        c_entries, c_counts = np.unique(label_arr[ind_corr], return_counts=True)\n","        skintone_counts_correct[c_entries] += c_counts\n","    print('\\n')\n","    for i in range(6):\n","        acc = skintone_counts_correct[i] / skintone_counts[i]\n","        print(f\"\\nAccuracy for skintone {i+1}: {acc}    (Counts {skintone_counts[i]})\")"],"metadata":{"id":"RmV9nh6A3pL1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bias_analysis(loaded_model, valid_loader)"],"metadata":{"id":"YJ3AqwVh3qUQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# with masking\n","train_transforms = transforms.Compose([\n","            transforms.Resize(256),\n","            transforms.RandomResizedCrop(224),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.RandomVerticalFlip(),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n","valid_transforms = transforms.Compose([\n","            transforms.Resize(256),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n","\n","train_dataset = CustomSkinDataset(data_path, train_df, train_transforms, mask=True)\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=2, sampler=BalancedBatchSampler(train_dataset, labels=np.array(train_df[\"fitzpatrick_scale\"])))\n","valid_dataset = CustomSkinDataset(data_path, valid_df, valid_transforms)\n","valid_loader = torch.utils.data.DataLoader(valid_dataset, shuffle=False, batch_size=batch_size, num_workers=2)\n","\n","model_ft = models.resnet18(pretrained=True)\n","num_ftrs = model_ft.fc.in_features\n","model_ft.fc = nn.Linear(num_ftrs, 6)\n","baseline_model = model_ft.to(device)\n","\n","loss_function = nn.CrossEntropyLoss()\n","\n","optimiser = optim.SGD(baseline_model.parameters(), lr=0.001, momentum=0.9)\n","\n","exp_lr_scheduler = lr_scheduler.StepLR(optimiser, step_size=10, gamma=0.1)"],"metadata":{"id":"ESae3Ts3PGDT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 10\n","best_acc, _ = validate(baseline_model, valid_loader, loss_function, 0, len_valid, batch_size)\n","for epoch in range(epochs):\n","    train(baseline_model, optimiser, exp_lr_scheduler, train_loader, loss_function, epoch+1, batch_size)\n","    acc, acc_give1 = validate(baseline_model, valid_loader, loss_function, epoch+1, len_valid, batch_size)\n","    if acc > best_acc:\n","        torch.save({'model':baseline_model.state_dict(), 'optimiser': optimiser.state_dict(), 'scheduler':exp_lr_scheduler.state_dict(), 'acc': acc, 'acc_give1': acc_give1, 'epoch': epoch+1}, f\"drive/MyDrive/Edinburgh/MLP/MLPcoursework4/models/maskedBaseline_epoch-{epoch+1}.chkpt\")\n","        best_acc = acc"],"metadata":{"id":"b8zMt9HKPWZx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for img, label in train_loader:\n","    plt.imshow(torchvision.utils.make_grid(img.add(1).mul(0.5)).clamp(0,1).cpu().data.permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\n","    plt.show()\n","    print(label + 1)\n","    break"],"metadata":{"id":"wfQhSBFk5pof"},"execution_count":null,"outputs":[]}]}