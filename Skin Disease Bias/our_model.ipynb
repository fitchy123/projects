{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"HmFAU59dW6XZ"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import random\n","import torch\n","from torch import Tensor\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import torch.nn.functional as F\n","from torchvision import transforms, models\n","from torch.utils.data import Dataset\n","from torchvision.io import read_image\n","from skimage import io, color\n","import os\n","from PIL import Image\n","import time, copy\n","from tqdm import tqdm\n","import cv2\n","import matplotlib.pyplot as plt\n","import torchvision\n","import math\n","import zipfile"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pn6L2pEgXRu5"},"outputs":[],"source":["#from google.colab import drive\n","#drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f-tbZPOayukd"},"outputs":[],"source":["# zip files into specified place on drive\n","\"\"\"import glob\n","\n","# Specify the folder containing .npy files and the destination ZIP file name\n","npy_folder_path = 'drive/MyDrive/Edinburgh/MLP/MLPcoursework4/image_arrays/'  # Update this path\n","zip_file_path = 'drive/MyDrive/Edinburgh/MLP/MLPcoursework4/image_arrays.zip'  # Update this path\n","\n","# Find all .npy files within the folder\n","npy_files = glob.glob(os.path.join(npy_folder_path, '*.npy'))\n","\n","batch = 0\n","batch_size = 50\n","num_batches = int(np.ceil(len(npy_files)/batch_size))\n","j = 0\n","for batch in range(num_batches):\n","    batch_files = npy_files[batch * batch_size:(batch * batch_size) + batch_size]\n","    if batch == 0:\n","        with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n","            for file in batch_files:\n","                # Add file to zip, preserving its folder structure\n","                zipf.write(file, os.path.relpath(file, os.path.dirname(npy_folder_path)))\n","                if j % 10 == 0: print(j)\n","                j+=1\n","    else:\n","        with zipfile.ZipFile(zip_file_path, 'a', zipfile.ZIP_DEFLATED) as zipf:\n","            for file in batch_files:\n","                # Add file to zip, preserving its folder structure\n","                #print(file)\n","                zipf.write(file, os.path.relpath(file, os.path.dirname(npy_folder_path)))\n","                if j % 10 == 0: print(j)\n","                j+=1\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FCGyRAKgZhak"},"outputs":[],"source":["# Path in your Google Drive\n","source_path = r'drive/MyDrive/Edinburgh/MLP/MLPcoursework4/image_arrays.zip'\n","# Destination path on the local VM disk\n","os.mkdir(\"/tmp/image_arrays\")\n","arr_zip = zipfile.ZipFile(source_path, \"r\")\n","arr_zip.extractall(\"tmp/image_arrays\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RtQ2e-JGXaxX"},"outputs":[],"source":["df = pd.read_csv(\"drive/MyDrive/Edinburgh/MLP/MLPcoursework4/fitzpatrick17k_filtered.csv\")\n","data_path = r\"tmp/image_arrays/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Obap25mhXd-y"},"outputs":[],"source":["# useful variables\n","img_height = 256\n","img_width = 256\n","batch_size  = 32\n","n_channels  = 3\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yv6jnugcXgtt"},"outputs":[],"source":["# split dataset\n","df = pd.read_csv(\"drive/MyDrive/Edinburgh/MLP/MLPcoursework4/fitzpatrick17k_filtered.csv\")\n","train_df = df[df[\"validation\"] == 0].copy()\n","train_df = train_df.reset_index(drop=True)\n","valid_df = df[df[\"validation\"] == 1].copy()\n","valid_df = valid_df.reset_index(drop=True)\n","len_train = len(train_df)\n","len_valid = len(valid_df)\n","batches_per_valid_epoch = np.ceil(len_valid / batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6VkayZPzX30B"},"outputs":[],"source":["# create dataset class\n","class CustomSkinDataset(Dataset):\n","    def __init__(self, img_dir, csv, transforms=None, colour_space=\"HSV\"):\n","        self.img_dir = img_dir\n","        self.colour_space = colour_space\n","        self.csv = csv\n","        self.transforms = transforms\n","    def __len__(self):\n","        return len(self.csv)\n","\n","    def __getitem__(self, idx):\n","        img = np.load(self.img_dir + self.csv.at[idx, 'md5hash'] + \".npy\")\n","        if(len(img.shape) < 3):\n","            img = color.gray2rgb(img)\n","        if self.colour_space == \"HSV\":\n","            img = cv2.cvtColor(img, cv2.COLOR_RGB2Lab)\n","        if self.transforms:\n","            img = self.transforms(Image.fromarray(img))\n","        label = self.csv[\"fitzpatrick_scale\"][idx] - 1\n","        return img[0].unsqueeze(0), img[1:], torch.tensor(label, dtype=torch.long)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"174MLANGZ2M8"},"outputs":[],"source":["# balanced batch sampler\n","class BalancedBatchSampler(torch.utils.data.sampler.Sampler):\n","    \"\"\"\n","    A pytorch dataset sampler to obtain balanced batches.\n","    Implementation from\n","    https://github.com/galatolofederico/pytorch-balanced-batch\n","    \"\"\"\n","\n","    def __init__(self, dataset, labels=None):\n","        self.labels = labels\n","        self.dataset = dict()\n","        self.balanced_max = 0\n","        # Save all the indices for all the classes\n","        for idx in range(0, len(dataset)):\n","            label = self._get_label(dataset, idx)\n","            if label not in self.dataset:\n","                self.dataset[label] = list()\n","            self.dataset[label].append(idx)\n","            # keep track of number in class with most entries\n","            self.balanced_max = (\n","                len(self.dataset[label])\n","                if len(self.dataset[label]) > self.balanced_max\n","                else self.balanced_max\n","            )\n","        # Oversample the classes with fewer elements than the max, creates balanced classes\n","        for label in self.dataset:\n","            while len(self.dataset[label]) < self.balanced_max:\n","                self.dataset[label].append(random.choice(self.dataset[label]))\n","        self.keys = list(self.dataset.keys())\n","        self.currentkey = 0\n","        self.indices = [-1] * len(self.keys)\n","\n","    def __iter__(self):\n","        i = 0\n","        while self.indices[self.currentkey] < self.balanced_max - 1:\n","            self.indices[self.currentkey] += 1\n","            yield self.dataset[self.keys[self.currentkey]][\n","                self.indices[self.currentkey]\n","            ]\n","            self.currentkey = (self.currentkey + 1) % len(self.keys)\n","            i += 1\n","        self.indices = [-1] * len(self.keys)\n","\n","    def _get_label(self, dataset, idx):\n","        if self.labels is not None:\n","            return self.labels[idx]\n","        else:\n","            return self.csv[\"fitzpatrick_scale\"][idx]\n","\n","    def __len__(self):\n","        return self.balanced_max * len(self.keys)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KZ5qn6RV2DOb"},"outputs":[],"source":["class Inception_first_three_layers(nn.Module):\n","    def __init__(self, id):\n","        super().__init__()\n","        self.init_first_three_layers(id)\n","\n","    def init_first_three_layers(self, id):\n","        if id == 0:\n","            num_in_channels = 192\n","        elif id == 1:\n","            num_in_channels = 256\n","        elif id == 2:\n","            num_in_channels = 288\n","        # Define convolutional layers\n","        self.conv_1x1 = nn.Conv2d(in_channels=num_in_channels, out_channels=64, kernel_size=1, stride=(1,1))\n","        self.bn1 = nn.BatchNorm2d(64)  # BatchNorm for conv_1x1\n","        self.conv_5x5 = nn.Conv2d(in_channels=num_in_channels, out_channels=48, kernel_size=1, stride=(1,1))\n","        self.bn2 = nn.BatchNorm2d(48)  # BatchNorm for conv_5x5\n","        self.conv_5x5_2 = nn.Conv2d(in_channels=48, out_channels=64, kernel_size=5, padding=2, stride=(1,1))\n","        self.bn3 = nn.BatchNorm2d(64)  # BatchNorm for conv_5x5_2\n","        self.conv3x3dbl = nn.Conv2d(in_channels=num_in_channels, out_channels=64, kernel_size=1, stride=(1,1))\n","        self.bn4 = nn.BatchNorm2d(64)  # BatchNorm for conv3x3dbl\n","        self.conv3x3dbl_2 = nn.Conv2d(in_channels=64, out_channels=96, kernel_size=3, padding=1, stride=(1,1))\n","        self.bn5 = nn.BatchNorm2d(96)  # BatchNorm for conv3x3dbl_2\n","        self.conv3x3dbl_3 = nn.Conv2d(in_channels=96, out_channels=96, kernel_size=3, padding=1, stride=(1,1))\n","        self.bn6 = nn.BatchNorm2d(96)  # BatchNorm for conv3x3dbl_3\n","        if id == 0:\n","            self.conv_pool = nn.Conv2d(in_channels=num_in_channels, out_channels=32, kernel_size=1, stride=(1,1))\n","        else:\n","            self.conv_pool = nn.Conv2d(in_channels=num_in_channels, out_channels=64, kernel_size=1, stride=(1,1))\n","        self.bn_pool = nn.BatchNorm2d(self.conv_pool.out_channels)  # BatchNorm for conv_pool\n","\n","    def forward(self, x):\n","        # Apply convolutional layers with BatchNorm and ReLU\n","        x1 = F.relu(self.bn1(self.conv_1x1(x)))\n","        x5 = F.relu(self.bn2(self.conv_5x5(x)))\n","        x5 = F.relu(self.bn3(self.conv_5x5_2(x5)))\n","        xdbl = F.relu(self.bn4(self.conv3x3dbl(x)))\n","        xdbl = F.relu(self.bn5(self.conv3x3dbl_2(xdbl)))\n","        xdbl = F.relu(self.bn6(self.conv3x3dbl_3(xdbl)))\n","        xpool = F.relu(self.bn_pool(self.conv_pool(F.max_pool2d(x, kernel_size=3, stride=1, padding=1))))\n","\n","        # Concatenate branches\n","        new_x = torch.cat((x1, x5, xdbl, xpool), dim=1)\n","        return new_x\n","\n","class Inception_fourth_layer(nn.Module):\n","    def __init__(self):\n","        super(Inception_fourth_layer, self).__init__()\n","        self.conv_3x3 = nn.Conv2d(in_channels=288, out_channels=384, kernel_size=3, stride=(2,2), padding=\"valid\")\n","        self.bn_3x3 = nn.BatchNorm2d(384)\n","\n","        self.conv3x3dbl = nn.Conv2d(in_channels=288, out_channels=64, kernel_size=1, stride=(1,1))\n","        self.bn3x3dbl = nn.BatchNorm2d(64)\n","\n","        self.conv3x3dbl_2 = nn.Conv2d(in_channels=64, out_channels=96, kernel_size=3, stride=(1,1), padding=\"same\")\n","        self.bn3x3dbl_2 = nn.BatchNorm2d(96)\n","\n","        self.conv3x3dbl_3 = nn.Conv2d(in_channels=96, out_channels=96, kernel_size=3, stride=(2,2), padding=\"valid\")\n","        self.bn3x3dbl_3 = nn.BatchNorm2d(96)\n","\n","    def forward(self, x):\n","        # Apply conv layers followed by batch normalization and ReLU\n","        x3 = F.relu(self.bn_3x3(self.conv_3x3(x)))\n","\n","        x3dbl = F.relu(self.bn3x3dbl(self.conv3x3dbl(x)))\n","        x3dbl = F.relu(self.bn3x3dbl_2(self.conv3x3dbl_2(x3dbl)))\n","        x3dbl = F.relu(self.bn3x3dbl_3(self.conv3x3dbl_3(x3dbl)))\n","\n","        # Branch pool\n","        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n","\n","        # Concatenate branches\n","        new_x = torch.cat([x3, x3dbl, branch_pool], dim=1)\n","        return new_x\n","\n","class Inception_last_two_layers(nn.Module):\n","    def __init__(self, id):\n","        super(Inception_last_two_layers, self).__init__()\n","        self.id = id\n","        if id == 4:\n","            num_output_features = 128\n","        else:\n","            num_output_features = 160\n","        # Define convolutional layers and corresponding batch normalization layers\n","        self.conv_1x1 = nn.Conv2d(in_channels=768, out_channels=192, kernel_size=1, stride=(1,1))\n","        self.bn_1x1 = nn.BatchNorm2d(192)\n","\n","        self.conv_7x7 = nn.Conv2d(in_channels=768, out_channels=num_output_features, kernel_size=1, stride=(1,1))\n","        self.bn_7x7 = nn.BatchNorm2d(num_output_features)\n","\n","        self.conv_7x7_2 = nn.Conv2d(in_channels=num_output_features, out_channels=num_output_features, kernel_size=(1,7), stride=(1,1), padding=\"same\")\n","        self.bn_7x7_2 = nn.BatchNorm2d(num_output_features)\n","\n","        self.conv_7x7_3 = nn.Conv2d(in_channels=num_output_features, out_channels=192, kernel_size=(7,1), stride=(1,1), padding=\"same\")\n","        self.bn_7x7_3 = nn.BatchNorm2d(192)\n","\n","        self.branch7x7dbl = nn.Conv2d(in_channels=768, out_channels=num_output_features, kernel_size=1, stride=(1,1))\n","        self.bn_branch7x7dbl = nn.BatchNorm2d(num_output_features)\n","\n","        self.branch7x7dbl_2 = nn.Conv2d(in_channels=num_output_features, out_channels=num_output_features, kernel_size=(7,1), stride=(1,1), padding=\"same\")\n","        self.bn_branch7x7dbl_2 = nn.BatchNorm2d(num_output_features)\n","\n","        self.branch7x7dbl_3 = nn.Conv2d(in_channels=num_output_features, out_channels=num_output_features, kernel_size=(1,7), stride=(1,1), padding=\"same\")\n","        self.bn_branch7x7dbl_3 = nn.BatchNorm2d(num_output_features)\n","\n","        self.branch7x7dbl_4 = nn.Conv2d(in_channels=num_output_features, out_channels=num_output_features, kernel_size=(7,1), stride=(1,1), padding=\"same\")\n","        self.bn_branch7x7dbl_4 = nn.BatchNorm2d(num_output_features)\n","\n","        self.branch7x7dbl_5 = nn.Conv2d(in_channels=num_output_features, out_channels=192, kernel_size=(1,7), stride=(1,1), padding=\"same\")\n","        self.bn_branch7x7dbl_5 = nn.BatchNorm2d(192)\n","\n","        self.conv_pool = nn.Conv2d(in_channels=768, out_channels=192, kernel_size=1, stride=(1,1))\n","        self.bn_conv_pool = nn.BatchNorm2d(192)\n","\n","        self.avg_pool = nn.AvgPool2d(kernel_size=3, stride=(1,1), padding=1)\n","\n","    def forward(self, x):\n","        if self.id == 4:\n","            num_output_features = 128\n","        else:\n","            num_output_features = 160\n","\n","        # Apply convolutional layers with corresponding batch normalization and ReLU activation\n","        x1 = F.relu(self.bn_1x1(self.conv_1x1(x)))\n","        x7 = F.relu(self.bn_7x7(self.conv_7x7(x)))\n","        x7 = F.relu(self.bn_7x7_2(self.conv_7x7_2(x7)))\n","        x7 = F.relu(self.bn_7x7_3(self.conv_7x7_3(x7)))\n","        xpool = self.avg_pool(x)\n","\n","        x7x7dbl = F.relu(self.bn_branch7x7dbl(self.branch7x7dbl(x)))\n","        x7x7dbl = F.relu(self.bn_branch7x7dbl_2(self.branch7x7dbl_2(x7x7dbl)))\n","        x7x7dbl = F.relu(self.bn_branch7x7dbl_3(self.branch7x7dbl_3(x7x7dbl)))\n","        x7x7dbl = F.relu(self.bn_branch7x7dbl_4(self.branch7x7dbl_4(x7x7dbl)))\n","        x7x7dbl = F.relu(self.bn_branch7x7dbl_5(self.branch7x7dbl_5(x7x7dbl)))\n","\n","        xpool = F.relu(self.bn_conv_pool((self.conv_pool(xpool))))\n","        # Concatenate branches\n","        new_x = torch.cat((x1, x7, x7x7dbl, xpool), dim=1)\n","        return new_x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MjRDqJ0gYBbc"},"outputs":[],"source":["class OurModel(nn.Module):\n","    def __init__(self, x, two_paths=True, inception=True) -> None:\n","        super().__init__()\n","        self.two_paths = two_paths\n","        self.inception = inception\n","        if two_paths:\n","            # L image convolution path\n","            self.l_conv1 = nn.Conv2d(in_channels=1, out_channels=math.floor(x/2), kernel_size=3, stride=(2,2), padding='valid')\n","            self.bn_l_conv1 = nn.BatchNorm2d(math.floor(x/2))\n","            self.l_conv2 = nn.Conv2d(in_channels=math.floor(x/2), out_channels=math.floor(x/2), kernel_size=3, padding='valid')\n","            self.bn_l_conv2 = nn.BatchNorm2d(math.floor(x/2))\n","            self.l_conv3 = nn.Conv2d(in_channels=math.floor(x/2), out_channels=x, kernel_size=3, padding=\"same\")\n","            self.bn_l_conv3 = nn.BatchNorm2d(x)\n","            self.l_max_pool = nn.MaxPool2d(kernel_size=3, stride=(2,2))\n","\n","            # AB image convolution path\n","            self.ab_conv1 = nn.Conv2d(in_channels=2, out_channels=32-math.floor(x/2), kernel_size=3, stride=(2,2), padding='valid')\n","            self.bn_ab_conv1 = nn.BatchNorm2d(32-math.floor(x/2))\n","            self.ab_conv2 = nn.Conv2d(in_channels=32-math.floor(x/2), out_channels=32-math.floor(x/2), kernel_size=3, padding='valid')\n","            self.bn_ab_conv2 = nn.BatchNorm2d(32-math.floor(x/2))\n","            self.ab_conv3 = nn.Conv2d(in_channels=32-math.floor(x/2), out_channels=64-x, kernel_size=3, padding=\"same\")\n","            self.bn_ab_conv3 = nn.BatchNorm2d(64-x)\n","            self.ab_max_pool = nn.MaxPool2d(kernel_size=3, stride=(2,2))\n","\n","            # combined\n","            self.conv4 = nn.Conv2d(in_channels=64, out_channels=80, kernel_size=1, stride=(1,1), padding='same')\n","\n","        else:\n","            self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=(2,2), padding='valid')\n","            self.bn_conv1 = nn.BatchNorm2d(64)\n","            self.max_pool = nn.MaxPool2d(kernel_size=3, stride=(2,2))\n","            self.conv4 = nn.Conv2d(in_channels=64, out_channels=80, kernel_size=1, stride=(1,1), padding='valid')\n","\n","        # Combined path\n","        self.bn_conv4 = nn.BatchNorm2d(80)\n","        self.conv5 = nn.Conv2d(in_channels=80, out_channels=192, kernel_size=3, padding='valid')\n","        self.bn_conv5 = nn.BatchNorm2d(192)\n","        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=(2,2))\n","\n","        if inception:\n","            # Inception X6\n","            self.Inception1 = Inception_first_three_layers(0)\n","            self.Inception2 = Inception_first_three_layers(1)\n","            self.Inception3 = Inception_first_three_layers(2)\n","            self.Inception4 = Inception_fourth_layer()\n","            self.Inception5 = Inception_last_two_layers(4)\n","            self.Inception6 = Inception_last_two_layers(5)\n","        else:\n","            self.conv6 = nn.Conv2d(in_channels=192, out_channels=384, kernel_size=1, stride=(1,1), padding='same')\n","            self.bn_conv6 = nn.BatchNorm2d(384)\n","            self.conv7 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=1, stride=(1,1), padding='same')\n","            self.bn_conv7 = nn.BatchNorm2d(384)\n","            self.conv8 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=1, stride=(1,1), padding='same')\n","            self.bn_conv8 = nn.BatchNorm2d(384)\n","            self.conv9 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=1, stride=(1,1), padding='same')\n","            self.bn_conv9 = nn.BatchNorm2d(384)\n","            self.conv10 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=1, stride=(1,1), padding='same')\n","            self.bn_conv10 = nn.BatchNorm2d(384)\n","            self.conv11 = nn.Conv2d(in_channels=384, out_channels=512, kernel_size=1, stride=(1,1), padding='same')\n","            self.bn_conv11 = nn.BatchNorm2d(512)\n","            self.conv12 = nn.Conv2d(in_channels=512, out_channels=768, kernel_size=1, stride=(2,2), padding='valid')\n","            self.bn_conv12 = nn.BatchNorm2d(768)\n","            self.conv13 = nn.Conv2d(in_channels=768, out_channels=768, kernel_size=1, stride=(1,1), padding='same')\n","            self.bn_conv13 = nn.BatchNorm2d(768)\n","            self.conv14 = nn.Conv2d(in_channels=768, out_channels=768, kernel_size=1, stride=(1,1), padding='same')\n","            self.bn_conv14 = nn.BatchNorm2d(768)\n","            self.conv15 = nn.Conv2d(in_channels=768, out_channels=768, kernel_size=1, stride=(1,1), padding='same')\n","            self.bn_conv15 = nn.BatchNorm2d(768)\n","            self.conv16 = nn.Conv2d(in_channels=768, out_channels=768, kernel_size=1, stride=(1,1), padding='same')\n","            self.bn_conv16 = nn.BatchNorm2d(768)\n","            self.conv17 = nn.Conv2d(in_channels=768, out_channels=768, kernel_size=1, stride=(1,1), padding='same')\n","            self.bn_conv17 = nn.BatchNorm2d(768)\n","            self.conv18 = nn.Conv2d(in_channels=768, out_channels=768, kernel_size=1, stride=(1,1), padding='same')\n","            self.bn_conv18 = nn.BatchNorm2d(768)\n","\n","\n","        self.globalAvgPooling = nn.AdaptiveAvgPool2d((1,1))\n","        self.denseLayer = nn.Linear(768, 6)\n","\n","    def forward(self, x_1, x_2):\n","        if self.two_paths:\n","            # L image convolution path\n","            x_1 = F.relu(self.bn_l_conv1(self.l_conv1(x_1)))\n","            x_1 = F.relu(self.bn_l_conv2(self.l_conv2(x_1)))\n","            x_1 = F.relu(self.bn_l_conv3(self.l_conv3(x_1)))\n","            x_1 = self.l_max_pool(x_1)\n","\n","            # AB image convolution path\n","            x_2 = F.relu(self.bn_ab_conv1(self.ab_conv1(x_2)))\n","            x_2 = F.relu(self.bn_ab_conv2(self.ab_conv2(x_2)))\n","            x_2 = F.relu(self.bn_ab_conv3(self.ab_conv3(x_2)))\n","            x_2 = self.ab_max_pool(x_2)\n","\n","            # Combined path\n","            x_combined = torch.cat((x_1, x_2), dim=1)\n","        else:\n","            x_combined = torch.cat((x_1, x_2), dim=1)\n","            x_combined = F.relu(self.bn_conv1(self.conv1(x_combined)))\n","            x_combined = self.max_pool(x_combined)\n","\n","        x_combined = F.relu(self.bn_conv4(self.conv4(x_combined)))\n","        x_combined = F.relu(self.bn_conv5(self.conv5(x_combined)))\n","        x_combined = self.max_pool(x_combined)\n","\n","        if self.inception:\n","            x_combined = self.Inception1(x_combined)\n","            x_combined = self.Inception2(x_combined)\n","            x_combined = self.Inception3(x_combined)\n","\n","            x_combined = self.Inception4(x_combined)\n","            x_combined = self.Inception5(x_combined)\n","            x_combined = self.Inception6(x_combined)\n","        else:\n","            x_combined = F.relu(self.bn_conv6(self.conv6(x_combined)))\n","            x_combined = F.relu(self.bn_conv7(self.conv7(x_combined)))\n","            x_combined = F.relu(self.bn_conv8(self.conv8(x_combined)))\n","            x_combined = F.relu(self.bn_conv9(self.conv9(x_combined)))\n","            x_combined = F.relu(self.bn_conv10(self.conv10(x_combined)))\n","            x_combined = F.relu(self.bn_conv11(self.conv11(x_combined)))\n","            x_combined = F.relu(self.bn_conv12(self.conv12(x_combined)))\n","            x_combined = F.relu(self.bn_conv13(self.conv13(x_combined)))\n","            x_combined = F.relu(self.bn_conv14(self.conv14(x_combined)))\n","            x_combined = F.relu(self.bn_conv15(self.conv15(x_combined)))\n","            x_combined = F.relu(self.bn_conv16(self.conv16(x_combined)))\n","            x_combined = F.relu(self.bn_conv17(self.conv17(x_combined)))\n","            x_combined = F.relu(self.bn_conv18(self.conv18(x_combined)))\n","\n","        x_combined = self.globalAvgPooling(x_combined)\n","        x_combined = self.denseLayer(x_combined.squeeze())\n","        return x_combined"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5q7jAX04ivxW"},"outputs":[],"source":["def print_model_parameters(model):\n","    total_params = sum(p.numel() for p in model.parameters())\n","    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    non_trainable_params = total_params - trainable_params\n","\n","    print(f\"Total Parameters: {total_params}\")\n","    print(f\"Trainable Parameters: {trainable_params}\")\n","    print(f\"Non-Trainable Parameters: {non_trainable_params}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KgWOPzukZ3XH"},"outputs":[],"source":["train_transforms = transforms.Compose([\n","            transforms.Resize(256),\n","            transforms.RandomResizedCrop(224),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.RandomVerticalFlip(),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n","valid_transforms = transforms.Compose([\n","            transforms.Resize(256),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n","\n","train_dataset = CustomSkinDataset(data_path, train_df, train_transforms, colour_space=\"HSV\")\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, pin_memory=True, num_workers=2,\n","                                           sampler=BalancedBatchSampler(train_dataset, labels=np.array(train_df[\"fitzpatrick_scale\"])),\n","                                           drop_last=False)\n","valid_dataset = CustomSkinDataset(data_path, valid_df, valid_transforms, colour_space=\"HSV\")\n","valid_loader = torch.utils.data.DataLoader(valid_dataset, shuffle=False, batch_size=batch_size, pin_memory=True, num_workers=2, drop_last=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SIqWI31qhafk"},"outputs":[],"source":["x = 11\n","our_model = OurModel(x, two_paths=True, inception=True)\n","our_model = our_model.to(device)\n","loss_function = nn.CrossEntropyLoss()\n","optimiser = optim.SGD(our_model.parameters(), lr=0.001, momentum=0.9)\n","scheduler = lr_scheduler.StepLR(optimiser, step_size=10, gamma=0.1)\n","print_model_parameters(our_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pQ_f57R4aZ4Z"},"outputs":[],"source":["for img_L, img_ab, label in train_loader:\n","    print(label + 1)\n","    plt.imshow(torchvision.utils.make_grid(img_L.unsqueeze(1).add(1).mul(0.5)).clamp(0,1).cpu().data.permute(0,2,1).contiguous().permute(2,1,0), cmap=\"gray\")\n","    plt.show()\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KzV5MlWKhcs6"},"outputs":[],"source":["def train(model, optim, train_loader, scheduler, loss_function, epoch, batch_size):\n","    model.train()\n","    total_loss = 0\n","    for img_l, img_ab, label in tqdm(train_loader, desc=f'Epoch {epoch}', total=len(train_loader)):\n","        img_l = img_l.to(device)\n","        img_ab = img_ab.to(device)\n","        label = label.to(device)\n","\n","        optim.zero_grad()\n","\n","        output = model(img_l, img_ab)\n","        loss = loss_function(output, label)\n","\n","        loss.backward()\n","        optim.step()\n","\n","        total_loss += loss.item()\n","\n","    scheduler.step()\n","    avg_loss = total_loss / len(train_loader)\n","    print(f\"Epoch: {epoch} ||| Training loss: {avg_loss}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H9-reeYJhdba"},"outputs":[],"source":["def validate(model, valid_loader, loss_function, epoch, valid_length, batch_size):\n","    model.eval()\n","    total_loss = 0\n","    num_correct, num_give1_correct = 0, 0\n","    for img_l, img_ab, label in tqdm(valid_loader, desc=f'Epoch {epoch}', total=len(valid_loader)):\n","        img_l = img_l.to(device)\n","        img_ab = img_ab.to(device)\n","        label = label.to(device)\n","\n","        output = model(img_l, img_ab)\n","        preds = torch.argmax(output, dim=1)\n","        num_correct += (preds == label).sum()\n","        num_give1_correct += (torch.logical_or(torch.logical_or((preds == label), (preds == (label - 1))), (preds == (label + 1)))).sum()\n","\n","        loss = loss_function(output, label)\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(valid_loader)\n","    acc = num_correct / valid_length\n","    acc_give1 = num_give1_correct / valid_length\n","    print(f\"Validation loss: {avg_loss} ||| Validation accuracy: {acc} ||| Validation +-1 accuracy: {acc_give1}\")\n","    return acc, acc_give1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TJLxBY-xhrxp"},"outputs":[],"source":["epochs = 30\n","best_acc, _ = validate(our_model, valid_loader, loss_function, 0, len_valid, batch_size)\n","for epoch in range(epochs):\n","    train(our_model, optimiser, train_loader, scheduler, loss_function, epoch+1, batch_size)\n","    acc, acc_give1 = validate(our_model, valid_loader, loss_function, epoch+1, len_valid, batch_size)\n","    if acc > best_acc:\n","        torch.save({'model':our_model.state_dict(), 'optimiser':optimiser.state_dict(), 'scheduler': scheduler, 'acc': acc, 'acc_give1': acc_give1, 'epoch': epoch+1}, f\"drive/MyDrive/Edinburgh/MLP/MLPcoursework4/models/ourModelAblationIncept.chkpt\")\n","        best_acc = acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XUsFprOQ-D8-"},"outputs":[],"source":["loaded_model = OurModel(x, two_paths=True)\n","loaded_model.to(device)\n","checkpoint = torch.load(\"drive/MyDrive/Edinburgh/MLP/MLPcoursework4/models/ourModel_x-11.chkpt\")\n","loaded_model.load_state_dict(checkpoint['model'])\n","_, _ = validate(loaded_model, valid_loader, loss_function, 23, len_valid, batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qk4QFFMGYMvV"},"outputs":[],"source":["def bias_analysis(model, valid_loader):\n","    model.eval()\n","    skintone_counts = np.zeros(6)\n","    skintone_counts_correct = np.zeros(6)\n","    for img_l, img_ab, label in tqdm(valid_loader, desc=f'Epoch {epoch}', total=len(valid_loader)):\n","        img_l = img_l.to(device)\n","        img_ab = img_ab.to(device)\n","        label = label.to(device)\n","\n","        output = model(img_l, img_ab)\n","        preds = torch.argmax(output, dim=1)\n","\n","        label_arr = label.cpu().detach().numpy()\n","        entries, counts = np.unique(label_arr, return_counts=True)\n","        skintone_counts[entries] += counts\n","        pred_arr = preds.cpu().detach().numpy()\n","        ind_corr = np.where(label_arr == pred_arr)\n","        c_entries, c_counts = np.unique(label_arr[ind_corr], return_counts=True)\n","        skintone_counts_correct[c_entries] += c_counts\n","    print('\\n')\n","    for i in range(6):\n","        acc = skintone_counts_correct[i] / skintone_counts[i]\n","        print(f\"\\nAccuracy for skintone {i}: {acc}    (Counts {skintone_counts[i]})\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2mKtN8ENiML7"},"outputs":[],"source":["bias_analysis(our_model, valid_loader)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}