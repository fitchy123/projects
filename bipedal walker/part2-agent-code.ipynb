{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD3_FORK",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTNU1mwGB1ZD"
      },
      "source": [
        "**Dependencies and setup**\n",
        "\n",
        "This can take a minute or so..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA38jtUgtZsG"
      },
      "source": [
        "# This code is based on:\n",
        "# Nikile Barhat. TD3-BipedalWalker-v2-PyTorch. url: https://github.com/nikhilbarhate99/TD3-PyTorch-BipedalWalker-v2. (accessed: 09.02.2022)\n",
        "# released under the MIT liscence\n",
        "# and:\n",
        "# Honghao Wei and Lei Ying. FORK. url: https://github.com/honghaow/FORK. (accessed: 09.02.2022). released under the MIT liscence\n",
        "\n",
        "%%capture\n",
        "\n",
        "!pip install 'gym[box2d]'\n",
        "!apt update\n",
        "!apt install xvfb -y\n",
        "!pip install pyvirtualdisplay\n",
        "\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as disp\n",
        "%matplotlib inline\n",
        "\n",
        "display = Display(visible=0,size=(600,600))\n",
        "display.start()\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "plot_interval = 10 # update the plot every N episodes\n",
        "video_every = 50 # videos can take a very long time to render so only do it every N episodes"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optional Google drive integration - this will allow you to save and resume training, and may speed up redownloading the dataset\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swJIZi4WkYAx",
        "outputId": "072cf3d8-6354-47d2-cb80-2ff757ec500d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Replay Buffer**"
      ],
      "metadata": {
        "id": "Jw62H7SSz3HN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=1e6):\n",
        "        self.buffer = []\n",
        "        self.max_size = int(max_size)\n",
        "        self.size = 0\n",
        "    \n",
        "    def add(self, transition):\n",
        "        self.size +=1\n",
        "        # transiton is tuple of (state, action, reward, next_state, done)\n",
        "        self.buffer.append(transition)\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        # delete 1/5th of the buffer when full\n",
        "        if self.size > self.max_size:\n",
        "            del self.buffer[0:int(self.size/5)]\n",
        "            self.size = len(self.buffer)\n",
        "        \n",
        "        indexes = np.random.randint(0, len(self.buffer), size=batch_size)\n",
        "        state, action, reward, next_state, done = [], [], [], [], []\n",
        "        \n",
        "        for i in indexes:\n",
        "            s, a, r, s_, d = self.buffer[i]\n",
        "            state.append(np.array(s, copy=False))\n",
        "            action.append(np.array(a, copy=False))\n",
        "            reward.append(np.array(r, copy=False))\n",
        "            next_state.append(np.array(s_, copy=False))\n",
        "            done.append(np.array(d, copy=False))\n",
        "        \n",
        "        return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(done)"
      ],
      "metadata": {
        "id": "DGns_-LLz2iU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJHtclV_30Re"
      },
      "source": [
        "**Reinforcement learning agent**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    #takes a state and outputs the action with the highest Q score\n",
        "    def __init__(self, state_dim, action_dim, max_action): \n",
        "        super(Actor, self).__init__()\n",
        "        \n",
        "        self.l1 = nn.Linear(state_dim, 256) \n",
        "        self.l2 = nn.Linear(256, 256)\n",
        "        self.l3 = nn.Linear(256, action_dim)\n",
        "        \n",
        "        self.max_action = max_action\n",
        "        \n",
        "    def forward(self, state):\n",
        "        a = F.relu(self.l1(state))\n",
        "        a = F.relu(self.l2(a))\n",
        "        a = torch.tanh(self.l3(a)) * self.max_action\n",
        "        return a\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    #takes state-action pair and outputs a Q score\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        \n",
        "        self.l1 = nn.Linear(state_dim + action_dim, 256)\n",
        "        self.l2 = nn.Linear(256, 256)\n",
        "        self.l3 = nn.Linear(256, 1)\n",
        "        \n",
        "    def forward(self, state, action):\n",
        "        state_action = torch.cat([state, action], 1)\n",
        "        \n",
        "        q = F.relu(self.l1(state_action))\n",
        "        q = F.relu(self.l2(q))\n",
        "        q = self.l3(q)\n",
        "        return q\n",
        "\n",
        "\n",
        "class SystemNet(nn.Module):\n",
        "    # takes a state-action pair and predicts the next state\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(SystemNet, self).__init__()\n",
        "        self.l1 = nn.Linear(state_size + action_size, 400)\n",
        "        self.l2 = nn.Linear(400, 300)\n",
        "        self.l3 = nn.Linear(300, state_size)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        xa = torch.cat([state, action], 1)\n",
        "\n",
        "        x1 = F.relu(self.l1(xa))\n",
        "        x1 = F.relu(self.l2(x1))\n",
        "        x1 = self.l3(x1)\n",
        "        return x1\n",
        "\n",
        "\n",
        "class RewardNet(nn.Module):\n",
        "    #takes a takes a state-action pair and the next state (st, at, st+1) and predicts the reward given\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(RewardNet, self).__init__()\n",
        "\n",
        "        # Q1 architecture\n",
        "        self.l1 = nn.Linear(2 * state_dim + action_dim, 256)\n",
        "        self.l2 = nn.Linear(256, 256)\n",
        "        self.l3 = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, state,next_state, action):\n",
        "        sa = torch.cat([state, next_state, action], 1)\n",
        "\n",
        "        q1 = F.relu(self.l1(sa))\n",
        "        q1 = F.relu(self.l2(q1))\n",
        "        q1 = self.l3(q1)\n",
        "        return q1\n",
        "\n",
        "\n",
        "class TD3:\n",
        "    def __init__(self, lr, state_dim, action_dim, max_action):\n",
        "        \n",
        "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
        "        \n",
        "        self.critic_1 = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_1_target = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_1_target.load_state_dict(self.critic_1.state_dict())\n",
        "        self.critic_1_optimizer = optim.Adam(self.critic_1.parameters(), lr=lr)\n",
        "        \n",
        "        self.critic_2 = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_2_target = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_2_target.load_state_dict(self.critic_2.state_dict())\n",
        "        self.critic_2_optimizer = optim.Adam(self.critic_2.parameters(), lr=lr)\n",
        "\n",
        "        self.system_net = SystemNet(state_dim, action_dim).to(device)\n",
        "        self.system_net_optimizer = optim.Adam(self.system_net.parameters(), lr=lr)\n",
        "        self.system_net.apply(self.init_weights)\n",
        "\n",
        "        self.reward_net = RewardNet(state_dim, action_dim).to(device)\n",
        "        self.reward_net_optimizer = torch.optim.Adam(self.reward_net.parameters(), lr=lr)\n",
        "\n",
        "        self.max_action = max_action\n",
        "        self.tot_updates = 0\n",
        "    \n",
        "    def init_weights(self,m):\n",
        "        # xavier initialisation for weights of system network\n",
        "        if type(m) == nn.Linear:\n",
        "          torch.nn.init.xavier_uniform_(m.weight)\n",
        "          m.bias.data.fill_(0.001)\n",
        "        \n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "        return self.actor(state).cpu().data.numpy().flatten()\n",
        "    \n",
        "    def update(self, replay_buffer, n_iter, batch_size, gamma, polyak, policy_noise, noise_clip, policy_delay, obs_upper_bound, obs_lower_bound, \n",
        "               sys_threshold, sys_weight):\n",
        "        \n",
        "        for i in range(n_iter):\n",
        "            self.tot_updates += 1\n",
        "            # Sample a batch of transitions from replay buffer:\n",
        "            state, action_, reward, next_state, done = replay_buffer.sample(batch_size)\n",
        "            state = torch.FloatTensor(state).to(device)\n",
        "            action = torch.FloatTensor(action_).to(device)\n",
        "            reward = torch.FloatTensor(reward).reshape((batch_size,1)).to(device)\n",
        "            next_state = torch.FloatTensor(next_state).to(device)\n",
        "            done = torch.FloatTensor(done).reshape((batch_size,1)).to(device)\n",
        "            \n",
        "            # Select next action according to target policy:\n",
        "            noise = torch.FloatTensor(action_).data.normal_(0, policy_noise).to(device)\n",
        "            noise = noise.clamp(-noise_clip, noise_clip)\n",
        "            next_action = (self.actor_target(next_state) + noise)\n",
        "            next_action = next_action.clamp(-self.max_action, self.max_action)\n",
        "            \n",
        "            # Compute target Q-value:\n",
        "            target_Q1 = self.critic_1_target(next_state, next_action)\n",
        "            target_Q2 = self.critic_2_target(next_state, next_action)\n",
        "            target_Q = torch.min(target_Q1, target_Q2)\n",
        "            target_Q = reward + ((1-done) * gamma * target_Q).detach()\n",
        "            \n",
        "            # Optimize Critic 1:\n",
        "            current_Q1 = self.critic_1(state, action)\n",
        "            loss_Q1 = F.mse_loss(current_Q1, target_Q)\n",
        "            self.critic_1_optimizer.zero_grad()\n",
        "            loss_Q1.backward()\n",
        "            self.critic_1_optimizer.step()\n",
        "            \n",
        "            # Optimize Critic 2:\n",
        "            current_Q2 = self.critic_2(state, action)\n",
        "            loss_Q2 = F.mse_loss(current_Q2, target_Q)\n",
        "            self.critic_2_optimizer.zero_grad()\n",
        "            loss_Q2.backward()\n",
        "            self.critic_2_optimizer.step()\n",
        "\n",
        "            # Train system net\n",
        "            predict_next_state = self.system_net(state, action)\n",
        "            predict_next_state = predict_next_state.clamp(obs_lower_bound, obs_upper_bound) # make sure state prediction is within state space\n",
        "            system_net_loss = F.smooth_l1_loss(predict_next_state, next_state.detach())\n",
        "            self.system_net_optimizer.zero_grad()\n",
        "            system_net_loss.backward()\n",
        "            self.system_net_optimizer.step()\n",
        "            self.system_net_loss = system_net_loss.item()\n",
        "\n",
        "            s_flag = 1 if system_net_loss.item() < sys_threshold else 0\n",
        "\n",
        "            # Train reward net\n",
        "            predict_reward = self.reward_net(state, next_state, action)\n",
        "            reward_net_loss = F.mse_loss(predict_reward, reward.detach())            \n",
        "            self.reward_net_optimizer.zero_grad()\n",
        "            reward_net_loss.backward()\n",
        "            self.reward_net_optimizer.step()\n",
        "            self.reward_net_loss = reward_net_loss.item()\n",
        "\n",
        "            # Delayed policy updates:\n",
        "            if self.tot_updates % policy_delay == 0:\n",
        "                # Compute actor loss:\n",
        "                actor_loss1 = -self.critic_1(state, self.actor(state)).mean()\n",
        "\n",
        "                # if system model loss below the system threshold use predicted future states and rewards to help train the actor\n",
        "                if s_flag == 1:\n",
        "                    # Predict the next state and action that would be taken in this state\n",
        "                    p_next_state = self.system_net(state, self.actor(state))\n",
        "                    p_next_state = p_next_state.clamp(obs_lower_bound, obs_upper_bound)\n",
        "                    actions2 = self.actor(p_next_state.detach())\n",
        "\n",
        "                    # Predict the reward given this\n",
        "                    p_next_r = self.reward_net(state, p_next_state.detach(), self.actor(state))\n",
        "                    \n",
        "                    # Repeat to get state and reward of st+2\n",
        "                    p_next_state2 = self.system_net(p_next_state, self.actor(p_next_state.detach()))\n",
        "                    p_next_state2 = p_next_state2.clamp(obs_lower_bound, obs_upper_bound)\n",
        "                    p_next_r2 = self.reward_net(p_next_state.detach(), p_next_state2.detach(), self.actor(p_next_state.detach()))\n",
        "                    actions3 = self.actor(p_next_state2.detach())\n",
        "\n",
        "                    # Calculate actor loss\n",
        "                    actor_loss2 =  self.critic_1(p_next_state2.detach(), actions3)\n",
        "                    actor_loss3 =  -(p_next_r + gamma * p_next_r2 + gamma ** 2 * actor_loss2).mean()\n",
        "                \n",
        "                    actor_loss = (actor_loss1 + sys_weight * actor_loss3)\n",
        "\n",
        "                else:\n",
        "                    actor_loss = actor_loss1\n",
        "                \n",
        "                # Optimize the actor\n",
        "                self.critic_1_optimizer.zero_grad()\n",
        "                self.critic_2_optimizer.zero_grad()\n",
        "                self.system_net_optimizer.zero_grad()\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.actor_optimizer.step()\n",
        "                \n",
        "                # Polyak averaging update:\n",
        "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
        "                \n",
        "                for param, target_param in zip(self.critic_1.parameters(), self.critic_1_target.parameters()):\n",
        "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
        "                \n",
        "                for param, target_param in zip(self.critic_2.parameters(), self.critic_2_target.parameters()):\n",
        "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
        "            \n",
        "                        \n",
        "    def save(self, directory, name):\n",
        "        torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, name))\n",
        "        torch.save(self.actor_target.state_dict(), '%s/%s_actor_target.pth' % (directory, name))\n",
        "        \n",
        "        torch.save(self.critic_1.state_dict(), '%s/%s_crtic_1.pth' % (directory, name))\n",
        "        torch.save(self.critic_1_target.state_dict(), '%s/%s_critic_1_target.pth' % (directory, name))\n",
        "        \n",
        "        torch.save(self.critic_2.state_dict(), '%s/%s_crtic_2.pth' % (directory, name))\n",
        "        torch.save(self.critic_2_target.state_dict(), '%s/%s_critic_2_target.pth' % (directory, name))\n",
        "\n",
        "        torch.save(self.system_net.state_dict(), '%s/%s_system_net.pth' % (directory, name))\n",
        "        \n",
        "        torch.save(self.reward_net.state_dict(), '%s/%s_reward_net.pth' % (directory, name))\n",
        "        \n",
        "    def load(self, directory, name):\n",
        "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "        self.actor_target.load_state_dict(torch.load('%s/%s_actor_target.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        self.critic_1.load_state_dict(torch.load('%s/%s_crtic_1.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "        self.critic_1_target.load_state_dict(torch.load('%s/%s_critic_1_target.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        self.critic_2.load_state_dict(torch.load( '%s/%s_crtic_2.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "        self.critic_2_target.load_state_dict(torch.load('%s/%s_critic_2_target.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "\n",
        "        self.system_net.load_state_dict(torch.load('%s/%s_system_net.pth' % (directory, name), map_location=lambda storage, loc: storage))\n",
        "\n",
        "        self.reward_net.load_state_dict(torch.load('%s/%s_reward_net.pth' % (directory, name), map_location=lambda storage, loc: storage))"
      ],
      "metadata": {
        "id": "RaR139DF0v8g"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEv4ZjXmyrHo"
      },
      "source": [
        "**Prepare the environment and wrap it to capture videos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Xrcek4hxDXl"
      },
      "source": [
        "%%capture\n",
        "env_name = \"BipedalWalker\"\n",
        "if env_name == \"BipedalWalker\":\n",
        "    env = gym.make(\"BipedalWalker-v3\")\n",
        "    max_reward = 320\n",
        "elif env_name == \"Pendulum\":\n",
        "    env = gym.make(\"Pendulum-v0\") # useful continuous environment for quick experiments\n",
        "    max_reward = -16.2736044\n",
        "elif env_name == \"BipedalWalkerHardcore\":\n",
        "    env = gym.make(\"BipedalWalkerHardcore-v3\") # only attempt this if your agent consistently aces BipedalWalker-v3\n",
        "    max_reward = 320\n",
        "\n",
        "env = gym.wrappers.Monitor(env, \"./video\", video_callable=lambda ep_id: ep_id%video_every == 0, force=True)\n",
        "\n",
        "obs_dim = env.observation_space.shape[0] #aka state dimensions\n",
        "act_dim = env.action_space.shape[0]\n",
        "act_max = float(env.action_space.high[0])\n",
        "obs_upper_bound = float(env.observation_space.high[0]) #state space upper bound\n",
        "obs_lower_bound = float(env.observation_space.low[0])  #state space lower bound\n",
        "if obs_upper_bound == float('inf'):\n",
        "\t\t\tobs_upper_bound, obs_lower_bound = 0, 0"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUw4h980jfnu",
        "outputId": "755712d5-763a-4cd7-ea7c-7615c8875221"
      },
      "source": [
        "print('The environment has {} observations and the agent can take {} actions'.format(obs_dim, act_dim))\n",
        "print('The device is: {}'.format(device))\n",
        "\n",
        "if device.type != 'cpu': print('It\\'s recommended to train on the cpu for this')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The environment has 24 observations and the agent can take 4 actions\n",
            "The device is: cuda\n",
            "It's recommended to train on the cpu for this\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.core.numeric import False_\n",
        "# in the submission please use seed 42 for verification\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "env.seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "env.action_space.seed(seed)\n",
        "\n",
        "# logging variables\n",
        "ep_reward = 0\n",
        "reward_list = []\n",
        "plot_data = []\n",
        "log_f = open(\"agent-log.txt\",\"w+\")\n",
        "tot_timesteps = 0\n",
        "total_reward_list = []\n",
        "flag = False\n",
        "\n",
        "# initialise agent\n",
        "reward_lower_bound = 0\n",
        "reward_upper_bound = 0\n",
        "sys_weight = 0.5\n",
        "sys_weight2 = 0.4\n",
        "sys_threshold = 0.010\n",
        "base_weight = 0.6\n",
        "\n",
        "max_episodes = 701\n",
        "max_timesteps = 2000\n",
        "gamma = 0.99                  # discount for future rewards\n",
        "batch_size = 100              # num of transitions sampled from replay buffer\n",
        "lr = 3e-4 #lr = 0.001\n",
        "exploration_noise = 0.1 \n",
        "polyak = 0.995                # target policy update parameter \n",
        "policy_noise = 0.2 * act_max  # target policy smoothing noise\n",
        "noise_clip = 0.5 * act_max\n",
        "policy_delay = 2              # delayed policy updates parameter\n",
        "directory = \"drive/My Drive/Colab Notebooks/DL and RL/RL coursework/TD3_FORK_\" + env_name # save trained models\n",
        "filename = \"{}_TD3_FORK\".format(env_name)\n",
        "reload = False\n",
        "\n",
        "replay_buffer = ReplayBuffer()\n",
        "agent = TD3(lr, obs_dim, act_dim, act_max)\n",
        "if reload: agent.load(directory, filename)\n",
        "\n",
        "# training procedure:\n",
        "for episode in range(1, max_episodes+1):\n",
        "    if episode > 10:\n",
        "        if np.mean(total_reward_list[-10:]) >= 300: flag = True\n",
        "    if flag == True: exploration_noise = max(exploration_noise - 0.001, 0)\n",
        "    \n",
        "    state = env.reset()\n",
        "\n",
        "    if env_name == \"BipedalWalkerHardcore\":\n",
        "        temporary_buffer = []\n",
        "\n",
        "    for t in range(max_timesteps):\n",
        "        tot_timesteps += 1\n",
        "\n",
        "        # random exploration for first 5 episodes\n",
        "        if episode <= 12:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            # select the agent action and add exploration noise\n",
        "            action = agent.select_action(state)\n",
        "            action = action + np.random.normal(0, act_max * exploration_noise, size=act_dim)\n",
        "            action = action.clip(env.action_space.low, env.action_space.high)\n",
        "\n",
        "        # take action in environment and get r and s'\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        if env_name == \"BipedalWalkerHardcore\":\n",
        "            # alter rewards for bipedal-walker hardcore\n",
        "            if reward == -100:\n",
        "                new_reward = -5\n",
        "            else:\n",
        "                new_reward = 5 * reward\n",
        "            \n",
        "            temporary_buffer.append((state, action, new_reward, next_state, float(done)))\n",
        "\n",
        "            if (done or t==(max_timesteps-1)):\n",
        "                if new_reward == -5:\n",
        "                    for mem in temporary_buffer:\n",
        "                        replay_buffer.add(mem)\n",
        "                # only add successful episode experiences 1/5th of the time\n",
        "                elif np.random.rand() > 0.8:  \n",
        "                    for mem in temporary_buffer:\n",
        "                        replay_buffer.add(mem)\n",
        "            ep_reward += reward\n",
        "            reward = new_reward\n",
        "        else:\n",
        "            replay_buffer.add((state, action, reward, next_state, float(done)))\n",
        "            ep_reward += reward\n",
        "        \n",
        "        state = next_state\n",
        "\n",
        "        # Update observation and reward bounds\n",
        "        obs_upper_bound = np.amax(state) if obs_upper_bound < np.amax(state) else obs_upper_bound\n",
        "        obs_lower_bound = np.amin(state) if obs_lower_bound > np.amin(state) else obs_lower_bound\n",
        "        reward_lower_bound = (reward) if reward_lower_bound > reward else reward_lower_bound\n",
        "        reward_upper_bound = (reward) if reward_upper_bound < reward else reward_upper_bound\n",
        "\n",
        "        # update agent\n",
        "        if episode > 12:\n",
        "                agent.update(replay_buffer, 1, batch_size, gamma, polyak, policy_noise, noise_clip, policy_delay, obs_upper_bound, obs_lower_bound, \n",
        "                             sys_threshold, sys_weight)\n",
        "        # stop iterating when the episode finished and update policy\n",
        "        if (done or t==(max_timesteps-1)):\n",
        "            # append the episode reward to the reward list\n",
        "            reward_list.append(ep_reward)\n",
        "            total_reward_list.append(ep_reward)\n",
        "            # update dynamic weight                    \n",
        "            sys_weight =  np.round((1 - np.clip(np.mean(reward_list[-100:])/max_reward, 0, 1)),4) * base_weight   \n",
        "            break                                      #tot_reward_list[-100:]\n",
        "\n",
        "\n",
        "    # save model periodically\n",
        "    if episode % 50 == 0:\n",
        "        agent.save(directory, filename)\n",
        "\n",
        "    # do NOT change this logging code - it is used for automated marking!\n",
        "    log_f.write('episode: {}, reward: {}\\n'.format(episode, ep_reward)) \n",
        "    log_f.flush()\n",
        "    ep_reward = 0\n",
        "    \n",
        "    # print reward data every so often - add a graph like this in your report\n",
        "    if episode % plot_interval == 0:\n",
        "        plot_data.append([episode, np.array(reward_list).mean(), np.array(reward_list).std()])\n",
        "        reward_list = []\n",
        "        # plt.rcParams['figure.dpi'] = 100\n",
        "        plt.plot([x[0] for x in plot_data], [x[1] for x in plot_data], '-', color='tab:grey')\n",
        "        plt.fill_between([x[0] for x in plot_data], [x[1]-x[2] for x in plot_data], [x[1]+x[2] for x in plot_data], alpha=0.2, color='tab:grey')\n",
        "        plt.xlabel('Episode number')\n",
        "        plt.ylabel('Episode reward')\n",
        "        plt.show()\n",
        "        disp.clear_output(wait=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "7uFoGhAJ2xpF",
        "outputId": "d9df7251-deed-4e0d-fcc9-b3b9def72279"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
